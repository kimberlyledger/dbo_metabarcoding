---
title: "PCR replicate decontamination and biological replicate summary"
author: "Kimberly Ledger"
date: "2024-07-02"
output: html_document
---

latest update: 27 Sept 2024

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

libraries
```{r}
library(tidyverse)
rename <- dplyr::rename
```

load sample type and other library prep info
```{r}
sample_metadata <- read.csv("/home/kimberly.ledger/dbo_metabarcoding/data/NBS_SBS_DBO_sample_names.csv")

#illumina output changed "_" to "-"
sample_metadata$sample_ID <- gsub("_", "-", sample_metadata$sample_ID) 
sample_metadata$sample_ID_date <- gsub("_", "-", sample_metadata$sample_ID_date) 
```

check sequence table outputs
```{r}
asv_table <- readRDS("/home/kimberly.ledger/dbo_metabarcoding/data/NBS_SBS_DBO_20240806/filtered.seqTab.RDS") %>%
  select(!Row.names)

#transpose 
asv_table <- data.frame(t(asv_table))

#set column names to be ASV# 
colnames(asv_table) <- asv_table["ASV",]

#remove row that has ASV#
asv_table <- asv_table[!rownames(asv_table) %in% c('ASV'), ]

#make sure reads are numbers
# Convert all character columns to numeric
for (col in names(asv_table)) {
  asv_table[[col]] <- as.numeric(asv_table[[col]])
}

#make make sample ID a column 
asv_table$sample_ID_date <- rownames(asv_table)

#rename the one sample that got the wrong ID in the sample sheet 
asv_table <- asv_table %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-A-20240423", "e00683-A-20240423", sample_ID_date)) %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-B-20240423", "e00683-B-20240423", sample_ID_date)) %>%
  mutate(sample_ID_date = ifelse(sample_ID_date == "e0683-C-20240423", "e00683-C-20240423", sample_ID_date))
```


add column to the ASV table that labels the sample type
```{r}
asv_table_with_sample_type <- sample_metadata %>%
  dplyr::select(sample_ID_date, sample_type, collection_year, project, seq_date) %>%
  left_join(asv_table, by = "sample_ID_date") %>%
  unite(col = "project_year", project, collection_year, sep = "_", remove = F)

# make a variable for the first and last ASV column in the table
asv_first <- which(colnames(asv_table_with_sample_type) == "ASV_0001")
asv_last <- ncol(asv_table_with_sample_type)
```


# account for likely contaminants 

- tag-jumping: this would be a run-specific process, so would need to separate the data by sequencing run. 
- also when considering what's in the negative PCR controls, that should be done on by-run basis.
- field negatives should be addressed on a project/year basis. 


## Step 1. Account for tag-jumping by using the positive controls 

subtract the proportion of reads that jumped into the positive control samples from each environmental sample 

identify the maximum proportion of reads for each ASV found in the positive controls
```{r}
prop_asvs_in_positives_20240423 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240423") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240423)
prop_asvs_in_positives_20240423
```
```{r}
prop_asvs_in_positives_20240509 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240509") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240509)
prop_asvs_in_positives_20240509
```

```{r}
prop_asvs_in_positives_20240611 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240611") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240611)
prop_asvs_in_positives_20240611
```

```{r}
prop_asvs_in_positives_20240613 <- asv_table_with_sample_type %>%
  filter(seq_date == "20240613") %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = 20240613)
prop_asvs_in_positives_20240613
```

```{r}
prop_asvs_in_positives_DBO21 <- asv_table_with_sample_type %>%
  filter(is.na(seq_date)) %>%
  filter(sample_type == "positive") %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads)) %>%
  mutate(Prop = reads/TotalReadsPerSample) %>%
  group_by(ASV) %>%
  summarise(max_prop = max(Prop)) %>%
  arrange(desc(max_prop)) %>%
  mutate(seq_date = NA)
prop_asvs_in_positives_DBO21 
```

DBO21 positive control was the Haast's eagle 

combine these tables
```{r}
prop_asvs_in_positives <- prop_asvs_in_positives_20240423 %>%
  bind_rows(prop_asvs_in_positives_20240509) %>%
  bind_rows(prop_asvs_in_positives_20240611) %>%
  bind_rows(prop_asvs_in_positives_20240613) %>%
  bind_rows(prop_asvs_in_positives_DBO21)
```


subtract the max proportion of tag-jumped reads for each ASV from samples
```{r}
indexhop_table <- asv_table_with_sample_type %>%
  pivot_longer(cols = c(asv_first:asv_last), names_to = "ASV", values_to = "reads") %>%
  mutate(reads = ifelse(is.na(reads), 0, reads)) %>%
  group_by(sample_ID_date) %>%
  mutate(TotalReadsPerSample = sum(reads, na.rm = T)) %>%
  left_join(prop_asvs_in_positives, by = c("ASV", "seq_date")) %>%
  mutate(IndexHoppingReads = TotalReadsPerSample*max_prop) %>%
  mutate(reads_IndexHop_removed = reads - IndexHoppingReads) %>%
  mutate(reads_IndexHop_removed = if_else(reads_IndexHop_removed < 0, 0, reads_IndexHop_removed))
head(indexhop_table)
```

clean up the table by removing columns no longer needed 
```{r}
asv_table_filter1 <- indexhop_table %>%
  dplyr::select(sample_ID_date, sample_type, project_year, collection_year, project, seq_date, ASV, reads_IndexHop_removed) %>%
  dplyr::rename(reads = reads_IndexHop_removed)
```

this is a summary of the number of reads removed by ASV and sample_ID
```{r}
decontaminated_1 <- indexhop_table %>%
  dplyr::select(sample_ID_date, sample_type, project_year, collection_year, project, seq_date, ASV, IndexHoppingReads) %>%
  filter(sample_type == "sample") %>%
  group_by(seq_date, ASV) %>%
  summarise(mean_reads = mean(IndexHoppingReads),
            reads_q.05 = quantile(IndexHoppingReads, probs=0.05),
            median_q.5 = median(IndexHoppingReads),
            reads_q.95 = quantile(IndexHoppingReads, probs=0.95)) %>%
  filter(mean_reads > 0) %>%
  filter(ASV != "ASV_0015")  ## remove the PC/sturgeon ASV
decontaminated_1  
```


## Step 2. Account for contaminants in positive and negative controls 

next we will remove ASVs that only occur in controls and not in environmental samples. 

number of reads
```{r}
reads_per_type_ASV <- asv_table_filter1 %>%
  group_by(ASV, sample_type) %>%
  summarize(TotalReadsPerASV = sum(reads, na.rm = TRUE)) %>%
  arrange(ASV)
```

what ASVs have no reads in samples, but reads in the controls? 
```{r}
not_in_samples <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
    filter(sample < 1)
not_in_samples
```


what ASVs do have reads in samples, but more reads in the controls? 
```{r}
more_in_pcr_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(pcr_blank > sample)
head(more_in_pcr_blanks)

more_in_extraction_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(extraction_blank > sample)
head(more_in_extraction_blanks)

more_in_pc_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(positive > sample)
head(more_in_pc_blanks)

more_in_fb_blanks <- reads_per_type_ASV %>%
  pivot_wider(names_from = "sample_type", values_from = c("TotalReadsPerASV")) %>%
  filter(sample > 1) %>%
  filter(field_blank > sample)
head(more_in_fb_blanks)
```


remove these from the asv table
```{r}
asv_table_filter2 <- asv_table_filter1 %>%
  filter(!ASV %in% not_in_samples$ASV) %>%
  filter(!ASV %in% more_in_pcr_blanks$ASV) %>%
  filter(!ASV %in% more_in_extraction_blanks$ASV) %>%
  filter(!ASV %in% more_in_pc_blanks$ASV) %>%
  filter(!ASV %in% more_in_fb_blanks$ASV)
```


## Step 3. Remove ASVs without taxonomic ID 

now lets see how many of these ASVs have taxonomic IDs (these are not final tax ids)
```{r}
taxonomy <- read.csv("/home/kimberly.ledger/dbo_metabarcoding/outputs/taxonomy_20240806_collapsed.csv") %>%
  select(!X) %>%
  rename(ASV = qseqid)
```

```{r}
asv_table_filter2_with_tax <- asv_table_filter2 %>%
  left_join(taxonomy)

# # Count occurrences of each key in table 1
# asv_table_filter2_counts <- asv_table_filter2 %>%
#   group_by(ASV) %>%
#   summarise(count = n())
# 
# # Count occurrences of each key in table 2
# taxonomy_counts <- taxonomy %>%
#   group_by(ASV) %>%
#   summarise(count = n())
# 
# # Identify keys that appear more than once in both tables
# many_to_many_keys <- inner_join(asv_table_filter2_counts %>% filter(count > 1),
#                                 taxonomy_counts %>% filter(count > 1),
#                                 by = "ASV")
# 
# # Print the keys with many-to-many relationships
# if (nrow(many_to_many_keys) > 0) {
#   print("Many-to-many relationships detected for the following keys:")
#   print(many_to_many_keys)
# } else {
#   print("No many-to-many relationships detected.")
# }
```

what ASV's do not have a taxonomic ID? 
```{r}
asv_table_filter2_with_tax %>%
  filter(is.na(taxon)) %>%
  group_by(ASV) %>%
  summarize(total_reads = sum(reads, na.rm = T))
```
double check the ASV's with >10,000 reads 

28 = human 
59 = Bos
63 = Atlantic cod - why did this not also match pollock/pcod?
88 = Liparis greeni
102 = Cottus 
131 = Sus

adding a few of these manually 
```{r}
asv_table_filter2_with_tax <- asv_table_filter2_with_tax %>%
  mutate(taxon = ifelse(ASV == "ASV_0063", "Gadus", taxon),
         taxonomic_level = ifelse(ASV == "ASV_0063", "genus", taxonomic_level),
         species = ifelse(ASV == "ASV_0063", NA, species),
         genus = ifelse(ASV == "ASV_0063", "Gadus", genus),
         family = ifelse(ASV == "ASV_0063", "Gadidae", family),
         order = ifelse(ASV == "ASV_0063", "Gadiformes", order),
         class = ifelse(ASV == "ASV_0063", "Actinopteri", class),
         phylum = ifelse(ASV == "ASV_0063", "Chordata", phylum),
         kingdom = ifelse(ASV == "ASV_0063", "Eukaryota", kingdom),
         taxon = ifelse(ASV == "ASV_0088", "Liparis greeni", taxon),
         taxonomic_level = ifelse(ASV == "ASV_0088", "species", taxonomic_level),
         species = ifelse(ASV == "ASV_0088", "Liparis greeni", species),
         genus = ifelse(ASV == "ASV_0088", "Liparis", genus),
         family = ifelse(ASV == "ASV_0088", "Liparidae", family),
         order = ifelse(ASV == "ASV_0088", "Perciformes", order),
         class = ifelse(ASV == "ASV_0088", "Actinopteri", class),
         phylum = ifelse(ASV == "ASV_0088", "Chordata", phylum),
         kingdom = ifelse(ASV == "ASV_0088", "Eukaryota", kingdom),
         taxon = ifelse(ASV == "ASV_0102", "Cottus", taxon),
         taxonomic_level = ifelse(ASV == "ASV_0102", "genus", taxonomic_level),
         species = ifelse(ASV == "ASV_0102", NA, species),
         genus = ifelse(ASV == "ASV_0102", "Cottus", genus),
         family = ifelse(ASV == "ASV_0102", "Cottidae", family),
         order = ifelse(ASV == "ASV_0102", "Perciformes", order),
         class = ifelse(ASV == "ASV_0102", "Actinopteri", class),
         phylum = ifelse(ASV == "ASV_0102", "Chordata", phylum),
         kingdom = ifelse(ASV == "ASV_0102", "Eukaryota", kingdom))
```

```{r}
asv_table_filter2_with_tax %>%
  filter(is.na(taxon)) %>%
  group_by(ASV) %>%
  summarize(total_reads = sum(reads, na.rm = T))
```

remove ASVs with no taxonomic id
```{r}
asv_table_filter3 <- asv_table_filter2_with_tax %>%
  filter(!is.na(taxon))
```


## Step 4. Consider what is still in the negative controls - on a run-by-run basis

```{r}
asv_table_filter3 %>%
  filter(sample_type %in% c("pcr_blank","extraction_blank")) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(sample_type~seq_date, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - pcr blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

keep this at the asv level
```{r}
asvs_PCRN <- asv_table_filter3 %>%
  filter(sample_type %in% c("pcr_blank","extraction_blank")) %>%
  filter(seq_date %in% c(20240611, 20240613)) %>%   ## just looking at runs with DBO samples here
  group_by(sample_type, seq_date, ASV) %>%
  summarise(total = sum(reads),
            max = max(reads),
            mean = mean(reads)) %>%
  arrange(desc(total)) %>%
  filter(total > 0)

asvs_PCRN
```
ASV2 - herring
ASV195 - herring
ASV3 - Stichaeidae
ASV1 - walleye pollock
ASV6 - arctic cod


I will NOT subtract any reads from samples based on the extraction or PCR negative controls. 
```{r}
# asvs_PCRN_mean <- asvs_PCRN %>%
#   select(!total) %>%
#   select(!max)
#   
# pcrn_table <- asv_table_filter3 %>%
#   left_join(asvs_PCRN_mean, by = c("seq_date", "ASV")) %>%
#   mutate(mean = ifelse(is.na(mean), 0, mean)) %>%
#   mutate(reads_pcrn_removed = reads - mean) %>%
#   mutate(reads_pcrn_removed = if_else(reads_pcrn_removed < 0, 0, reads_pcrn_removed))
# pcrn_table
```

clean up the table by removing columns no longer needed 
```{r}
# asv_table_filter4 <- pcrn_table %>%
#   select(!reads) %>%
#   select(!mean) %>%
#   dplyr::rename(reads = reads_pcrn_removed)
```


## Step 5.  Address field negatives. 

- field negatives should be addressed on a project/year basis.

plot by project and year 
```{r}
asv_table_filter4 %>%
  filter(sample_type == "field_blank") %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  theme_bw() +
  facet_grid(~project_year, scales = "free_x") + 
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "ASV reads - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "none",
    legend.title = element_blank()
  )
```

look closer at DBO 2023
```{r}
asv_table_filter3 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "DBO_2023") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "DBO 2023 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```

three pcr reps of each field blank extraction. only field blank has lots of seqs in all three pcr reps. will need to report. 

look closer at NBS 2021 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2021") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2021 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at NBS 2022 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2022") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2022 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at NBS 2023 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "NBS_2023") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "NBS 2023 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


look closer at SBS 2022 
```{r}
asv_table_filter4 %>%
  separate(sample_ID_date, into = c("extraction_ID", "replicate", "seq_date2"),  remove = F) %>%
  filter(sample_type == "field_blank") %>%
  filter(project_year == "SBS_2022") %>%
  filter(reads > 0) %>%
  ggplot(aes(x=sample_ID_date, y=reads, fill=taxon)) +
  geom_bar(stat = "identity") + 
  facet_grid(~extraction_ID, scales = "free_x") + 
  theme_bw() +
  labs(
    y = "number of sequencing reads",
    x = "sample ID",
    title = "SBS 2022 - field blanks") + 
  theme(
    axis.text.x = element_text(angle = 90, hjust = 0.95),
    legend.text = element_text(size = 8),
    legend.key.size = unit(0.3, "cm"),
    legend.position = "right",
    legend.title = element_blank()
  )
```


alrighty, depending on how i end up handling field samples, the high dissimilarity and single pcr reps with seqs from field blanks may be able to stand out from the real samples.  

no extraction or field negatives sequenced with 2021 DBO data 

```{r}
write.csv(asv_table_filter3, "/home/kimberly.ledger/dbo_metabarcoding/outputs/decontaminated_asv_table.csv")
```


